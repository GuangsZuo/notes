版本：
    hadoop 2.7.3, hive-2.1.1, spark-1.6.3-hadoop2.6  (spark1.6 只支持到hdp2.6， 后来这套东西搭起来发现spark 写到hdfs的orc文件， hive 读不了，可能由于这个原因， 可以试试把hdp降到2.6 试试）
hadoop
  1）prerequired Java 1.8 
        export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home"
       执行/usr/libexec/java_home去查看自己的java_home
       prerequired  尝试ssh localhost 是否可以
      如果不可以：
              system preferences-> sharing   enable remote login
              并设置免密登录
               $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
               $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
  2）安装好之后，配置path 
      export PATH="$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin"
      HADOOP_HOME 为hadoop的安装路径
  3)  参考hdp官方文档 配置伪dfs http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html
       配置好后执行  
            $ hdfs namenode -format
            $ sh start-dfs.sh
      现在可以在 http://localhost:50070 查看  
hive 
     hive安装好之后，需要配置hive-site.xml
          与metastore相关      refer ：https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin
              javax.jdo.option.ConnectionURL ： jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true
              javax.jdo.option.ConnectionDriverName ： com.mysql.jdbc.Driver   可能需要将jdbc的jar包放到 HIVE_HOME/lib 下  
              javax.jdo.option.ConnectionUserName ： <connection-user-name>
              javax.jdo.option.ConnectionPassword : <connection-user-password>
             hive.metastore.schema.verification : false                   
spark
        编辑spark/conf 下的spark-env.sh
             export HADOOP_HOME=<your-hadoop-install-path>
           export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop"

